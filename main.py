from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
import time
import json
import pandas as pd
import os
import random
from tqdm import tqdm
import multiprocessing
from multiprocessing import Pool, Manager, Lock
import functools

def setup_driver():
    """Kh·ªüi t·∫°o tr√¨nh duy·ªát Chrome"""
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("--log-level=3")  # T·∫Øt log c·ªßa Chrome
    options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
    options.add_experimental_option("useAutomationExtension", False)
    
    # User-Agent gi·ªëng tr√¨nh duy·ªát th·∫≠t
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    
    # Gi·∫£ m·∫°o th√¥ng tin webdriver
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    return driver

def crawl_property_info(url):
    """C√†o th√¥ng tin b·∫•t ƒë·ªông s·∫£n t·ª´ URL c·ª• th·ªÉ"""
    driver = setup_driver()
    property_data = {}
    
    try:
        driver.get(url)
        wait_time = random.uniform(1, 1.5)
        # ƒê·ª£i trang t·∫£i xong
        time.sleep(wait_time)
        
        # T√¨m t·∫•t c·∫£ c√°c th·∫ª div c√≥ class="re__pr-specs-content-item"
        spec_items = driver.find_elements(By.CSS_SELECTOR, "div.re__pr-specs-content-item")
        
        for item in spec_items:
            text = item.text.strip()
            if "\n" in text:
                # T√°ch d√≤ng ƒë·∫ßu ti√™n l√†m t√™n thu·ªôc t√≠nh, ph·∫ßn c√≤n l·∫°i l√† gi√° tr·ªã
                parts = text.split("\n", 1)
                property_name = parts[0].strip()
                property_value = parts[1].strip()
                property_data[property_name] = property_value
        
        # T√¨m th·∫ª div c√≥ class="re__pr-short-info-item js__pr-config-item"
        short_info_items = driver.find_elements(By.CSS_SELECTOR, "div.re__pr-short-info-item.js__pr-config-item")
        
        for item in short_info_items:
            text = item.text.strip()
            if "\n" in text:
                parts = text.split("\n", 1)
                property_name = parts[0].strip()
                property_value = parts[1].strip()
                property_data[property_name] = property_value
        
        return property_data, None
            
    except Exception as e:
        error_message = str(e)
        return {"URL": url, "Error": error_message}, error_message
    finally:
        driver.quit()

def read_urls_from_csv(csv_file='linkProduct.csv'):
    """ƒê·ªçc danh s√°ch URL t·ª´ file CSV"""
    try:
        # Ki·ªÉm tra n·∫øu file t·ªìn t·∫°i
        if not os.path.exists(csv_file):
            print(f"File {csv_file} kh√¥ng t·ªìn t·∫°i.")
            
            # Ki·ªÉm tra xem c√≥ file JSON kh√¥ng ƒë·ªÉ chuy·ªÉn ƒë·ªïi
            json_file = csv_file.replace('.csv', '.json')
            if os.path.exists(json_file):
                print(f"T√¨m th·∫•y file JSON {json_file}, ƒëang chuy·ªÉn ƒë·ªïi th√†nh CSV...")
                with open(json_file, 'r', encoding='utf-8') as f:
                    urls = json.load(f)
                
                df = pd.DataFrame({'url': urls})
                df.to_csv(csv_file, index=False)
                print(f"ƒê√£ chuy·ªÉn ƒë·ªïi th√†nh c√¥ng {len(urls)} URL sang file {csv_file}")
                return urls
            return []
            
        # ƒê·ªçc file CSV
        df = pd.read_csv(csv_file)
        
        # T√¨m c·ªôt ch·ª©a URL
        url_column = None
        for col in ['url', 'URL', 'link', 'Link']:
            if col in df.columns:
                url_column = col
                break
        
        if not url_column and len(df.columns) > 0:
            url_column = df.columns[0]
        
        if url_column:
            urls = df[url_column].tolist()
            print(f"ƒê√£ ƒë·ªçc {len(urls)} URL t·ª´ file {csv_file}")
            return urls
        else:
            print(f"Kh√¥ng t√¨m th·∫•y c·ªôt URL trong file {csv_file}")
            return []
            
    except Exception as e:
        print(f"L·ªói khi ƒë·ªçc file CSV: {e}")
        return []

def append_to_csv_safe(df, output_file, file_lock, batch_info):
    """Ghi DataFrame v√†o file CSV v·ªõi kh√≥a ƒë·ªìng b·ªô h√≥a gi·ªØa c√°c ti·∫øn tr√¨nh"""
    # S·ª≠ d·ª•ng kh√≥a ƒë·ªÉ ƒë·∫£m b·∫£o ch·ªâ m·ªôt ti·∫øn tr√¨nh ghi file t·∫°i m·ªôt th·ªùi ƒëi·ªÉm
    with file_lock:
        process_id, batch_id = batch_info
        batch_count = f"{process_id}-{batch_id}"
        
        # Ki·ªÉm tra xem file ƒë√£ t·ªìn t·∫°i ch∆∞a
        file_exists = os.path.exists(output_file) and os.path.getsize(output_file) > 0
        
        if file_exists:
            try:
                # ƒê·ªçc header c·ªßa file hi·ªán c√≥ ƒë·ªÉ ki·ªÉm tra c·∫•u tr√∫c
                existing_df = pd.read_csv(output_file)
                
                # So s√°nh c·∫•u tr√∫c c·ªôt
                existing_cols = set(existing_df.columns)
                new_cols = set(df.columns)
                
                if existing_cols != new_cols:
                    print(f"[Ti·∫øn tr√¨nh {process_id}] C·∫•u tr√∫c c·ªôt kh√°c nhau. ƒêang k·∫øt h·ª£p d·ªØ li·ªáu...")
                    # K·∫øt h·ª£p DataFrame m·ªõi v·ªõi d·ªØ li·ªáu hi·ªán c√≥
                    combined_df = pd.concat([existing_df, df], ignore_index=True)
                    
                    # Ghi l·∫°i to√†n b·ªô file
                    combined_df.to_csv(output_file, index=False, encoding='utf-8-sig')
                    print(f"[Ti·∫øn tr√¨nh {process_id}] üíæ ƒê√£ l∆∞u l√¥ {batch_count}: {len(df)} b·∫£n ghi m·ªõi, t·ªïng {len(combined_df)} b·∫£n ghi")
                    return
                
                # N·∫øu c·∫•u tr√∫c c·ªôt gi·ªëng nhau, ch·ªâ c·∫ßn append
                df.to_csv(output_file, mode='a', header=False, index=False, encoding='utf-8-sig')
                print(f"[Ti·∫øn tr√¨nh {process_id}] üíæ ƒê√£ l∆∞u l√¥ {batch_count}: {len(df)} b·∫£n ghi")
                
            except Exception as e:
                print(f"[Ti·∫øn tr√¨nh {process_id}] ‚ùå L·ªói khi x·ª≠ l√Ω file: {e}")
                # N·∫øu c√≥ l·ªói, ghi ƒë√® file
                df.to_csv(output_file, index=False, encoding='utf-8-sig')
                print(f"[Ti·∫øn tr√¨nh {process_id}] ƒê√£ ghi ƒë√® file v·ªõi l√¥ {batch_count}")
        else:
            # N·∫øu file ch∆∞a t·ªìn t·∫°i, t·∫°o m·ªõi
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"[Ti·∫øn tr√¨nh {process_id}] üíæ ƒê√£ t·∫°o file m·ªõi v√† l∆∞u {len(df)} b·∫£n ghi v√†o {output_file}")

def process_url_batch(batch_data):
    """H√†m x·ª≠ l√Ω cho m·ªói ti·∫øn tr√¨nh"""
    process_id, url_batch, output_file, file_lock = batch_data
    
    print(f"[Ti·∫øn tr√¨nh {process_id}] B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(url_batch)} URLs")
    batch_size = 10  # Ghi file sau m·ªói 10 URL
    total_urls = len(url_batch)
    results = []
    
    # X·ª≠ l√Ω URLs theo batch nh·ªè
    for i, url in enumerate(url_batch):
        position = i + 1
        print(f"[Ti·∫øn tr√¨nh {process_id}] [{position}/{total_urls}] ƒêang x·ª≠ l√Ω: {url}")
        
        property_data, error = crawl_property_info(url)
        
        if error:
            print(f"[Ti·∫øn tr√¨nh {process_id}] ‚ùå L·ªói: {error}")
        else:
            results.append(property_data)
            print(f"[Ti·∫øn tr√¨nh {process_id}] ‚úÖ X·ª≠ l√Ω th√†nh c√¥ng")
        
        # Ghi k·∫øt qu·∫£ sau m·ªói batch_size ho·∫∑c khi ƒë·∫øn URL cu·ªëi c√πng
        if len(results) >= batch_size or i == total_urls - 1:
            if results:
                batch_id = i // batch_size + 1
                batch_info = (process_id, batch_id)
                df = pd.DataFrame(results)
                append_to_csv_safe(df, output_file, file_lock, batch_info)
                results = []  # Reset danh s√°ch k·∫øt qu·∫£ sau khi ghi
    
    print(f"[Ti·∫øn tr√¨nh {process_id}] Ho√†n th√†nh x·ª≠ l√Ω t·∫•t c·∫£ URLs")
    return process_id

def process_with_multiprocessing(urls, num_processes=4, output_file='property_data.csv'):
    """X·ª≠ l√Ω c√°c URL v·ªõi ƒëa ti·∫øn tr√¨nh"""
    # Kh·ªüi t·∫°o Manager ƒë·ªÉ chia s·∫ª kh√≥a gi·ªØa c√°c ti·∫øn tr√¨nh
    with Manager() as manager:
        file_lock = manager.Lock()
        
        # Chia URLs th√†nh c√°c nh√≥m cho t·ª´ng ti·∫øn tr√¨nh
        chunk_size = len(urls) // num_processes
        url_batches = []
        
        for i in range(num_processes):
            start_idx = i * chunk_size
            # ƒê·ªëi v·ªõi ti·∫øn tr√¨nh cu·ªëi c√πng, l·∫•y t·∫•t c·∫£ c√°c URL c√≤n l·∫°i
            end_idx = (i + 1) * chunk_size if i < num_processes - 1 else len(urls)
            url_batches.append((i + 1, urls[start_idx:end_idx], output_file, file_lock))
        
        print(f"Kh·ªüi t·∫°o {num_processes} ti·∫øn tr√¨nh, m·ªói ti·∫øn tr√¨nh x·ª≠ l√Ω {chunk_size} URLs")
        
        # Kh·ªüi t·∫°o v√† ch·∫°y c√°c ti·∫øn tr√¨nh
        with Pool(processes=num_processes) as pool:
            # B·∫Øt ƒë·∫ßu x·ª≠ l√Ω v√† ch·ªù k·∫øt qu·∫£
            for process_id in pool.imap_unordered(process_url_batch, url_batches):
                print(f"Ti·∫øn tr√¨nh {process_id} ƒë√£ ho√†n th√†nh")

if __name__ == "__main__":
    # File ƒë·∫ßu v√†o v√† ƒë·∫ßu ra    
    input_csv = 'linkProduct.csv'
    output_csv = 'property_data.csv'
    num_processes = 8  # S·ªë ti·∫øn tr√¨nh x·ª≠ l√Ω ƒë·ªìng th·ªùi
    
    # Hi·ªÉn th·ªã ti√™u ƒë·ªÅ
    print("\n" + "="*70)
    print("üè† B·∫ÆT ƒê·∫¶U C√ÄO D·ªÆ LI·ªÜU B·∫§T ƒê·ªòNG S·∫¢N ƒêA TI·∫æN TR√åNH")
    print("="*70 + "\n")
    
    # ƒê·ªçc c√°c URL t·ª´ file CSV
    print("üìÇ ƒêang ƒë·ªçc danh s√°ch URL...")
    urls = read_urls_from_csv(input_csv)
    
    if urls:
        print(f"üîç ƒê√£ t√¨m th·∫•y {len(urls)} URL ƒë·ªÉ x·ª≠ l√Ω v·ªõi {num_processes} ti·∫øn tr√¨nh\n")
        
        # Ghi l·∫°i th·ªùi gian b·∫Øt ƒë·∫ßu
        start_time = time.time()
        
        # X·ª≠ l√Ω URL v·ªõi ƒëa ti·∫øn tr√¨nh
        process_with_multiprocessing(urls[2924:], num_processes, output_csv)
        
        # T√≠nh th·ªùi gian th·ª±c hi·ªán
        elapsed_time = time.time() - start_time
        hours, remainder = divmod(elapsed_time, 3600)
        minutes, seconds = divmod(remainder, 60)
        
        # Hi·ªÉn th·ªã th√¥ng b√°o ho√†n th√†nh
        print("\n" + "="*70)
        print(f"‚úÖ HO√ÄN TH√ÄNH! Th·ªùi gian: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}")
        print(f"üìä D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {output_csv}")
        print("="*70)
    else:
        print("‚ùå Kh√¥ng c√≥ URL ƒë·ªÉ x·ª≠ l√Ω.")