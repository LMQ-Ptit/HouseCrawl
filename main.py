from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
import time
import json
import pandas as pd
import os
import random
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
import threading

# Kh√≥a ƒë·ªÉ ƒë·ªìng b·ªô h√≥a ghi v√†o file CSV
file_lock = threading.Lock()

def setup_driver():
    """Kh·ªüi t·∫°o tr√¨nh duy·ªát Chrome"""
    options = Options()
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option("useAutomationExtension", False)
    
    # User-Agent gi·ªëng tr√¨nh duy·ªát th·∫≠t
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    
    # Gi·∫£ m·∫°o th√¥ng tin webdriver
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    
    return driver

def crawl_property_info(url, position=0, total=0):
    """C√†o th√¥ng tin b·∫•t ƒë·ªông s·∫£n t·ª´ URL c·ª• th·ªÉ"""
    driver = setup_driver()
    property_data = {}  
    
    try:
        tqdm.write(f"[{position}/{total}] ƒêang truy c·∫≠p: {url}")
        driver.get(url)
        wait_time = random.uniform(1, 1.5)
        # ƒê·ª£i trang t·∫£i xong
        time.sleep(wait_time)
        
        # T√¨m t·∫•t c·∫£ c√°c th·∫ª div c√≥ class="re__pr-specs-content-item"
        spec_items = driver.find_elements(By.CSS_SELECTOR, "div.re__pr-specs-content-item")
        
        for item in spec_items:
            text = item.text.strip()
            if "\n" in text:
                # T√°ch d√≤ng ƒë·∫ßu ti√™n l√†m t√™n thu·ªôc t√≠nh, ph·∫ßn c√≤n l·∫°i l√† gi√° tr·ªã
                parts = text.split("\n", 1)
                property_name = parts[0].strip()
                property_value = parts[1].strip()
                property_data[property_name] = property_value
        
        # T√¨m th·∫ª div c√≥ class="re__pr-short-info-item js__pr-config-item"
        short_info_items = driver.find_elements(By.CSS_SELECTOR, "div.re__pr-short-info-item.js__pr-config-item")
        
        for item in short_info_items:
            text = item.text.strip()
            if "\n" in text:
                parts = text.split("\n", 1)
                property_name = parts[0].strip()
                property_value = parts[1].strip()
                property_data[property_name] = property_value
        
        # Chuy·ªÉn dictionary th√†nh DataFrame
        df = pd.DataFrame([property_data])
        return df, None  # Tr·∫£ v·ªÅ DataFrame v√† kh√¥ng c√≥ l·ªói
            
    except Exception as e:
        error_msg = str(e)
        tqdm.write(f"  ‚ùå L·ªói: {error_msg}")
        # V·∫´n tr·∫£ v·ªÅ DataFrame v·ªõi URL ƒë·ªÉ ghi l·∫°i th√¥ng tin th·∫•t b·∫°i
        return pd.DataFrame([{"URL": url, "Error": error_msg}]), error_msg
    finally:
        driver.quit()

def read_urls_from_csv(csv_file='linkProduct.csv'):
    """ƒê·ªçc danh s√°ch URL t·ª´ file CSV"""
    try:
        # Ki·ªÉm tra n·∫øu file t·ªìn t·∫°i
        if not os.path.exists(csv_file):
            print(f"File {csv_file} kh√¥ng t·ªìn t·∫°i.")
            
            # Ki·ªÉm tra xem c√≥ file JSON kh√¥ng ƒë·ªÉ chuy·ªÉn ƒë·ªïi
            json_file = csv_file.replace('.csv', '.json')
            if os.path.exists(json_file):
                print(f"T√¨m th·∫•y file JSON {json_file}, ƒëang chuy·ªÉn ƒë·ªïi th√†nh CSV...")
                with open(json_file, 'r', encoding='utf-8') as f:
                    urls = json.load(f)
                
                df = pd.DataFrame({'url': urls})
                df.to_csv(csv_file, index=False)
                print(f"ƒê√£ chuy·ªÉn ƒë·ªïi th√†nh c√¥ng {len(urls)} URL sang file {csv_file}")
                return urls
            return []
            
        # ƒê·ªçc file CSV
        df = pd.read_csv(csv_file)
        
        # T√¨m c·ªôt ch·ª©a URL (c√≥ th·ªÉ l√† 'url', 'link', ho·∫∑c c·ªôt ƒë·∫ßu ti√™n)
        url_column = None
        for col in ['url', 'URL', 'link', 'Link']:
            if col in df.columns:
                url_column = col
                break
        
        if not url_column and len(df.columns) > 0:
            # N·∫øu kh√¥ng t√¨m th·∫•y c·ªôt t√™n c·ª• th·ªÉ, s·ª≠ d·ª•ng c·ªôt ƒë·∫ßu ti√™n
            url_column = df.columns[0]
        
        if url_column:
            urls = df[url_column].tolist()
            print(f"ƒê√£ ƒë·ªçc {len(urls)} URL t·ª´ file {csv_file}")
            return urls
        else:
            print(f"Kh√¥ng t√¨m th·∫•y c·ªôt URL trong file {csv_file}")
            return []
            
    except Exception as e:
        print(f"L·ªói khi ƒë·ªçc file CSV: {e}")
        return []

def append_to_csv(df, output_file='property_data.csv', batch_count=1):
    """
    Ghi DataFrame v√†o file CSV ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch v·ªõi d·ªØ li·ªáu hi·ªán c√≥
    
    Parameters:
        df (DataFrame): DataFrame c·∫ßn ghi
        output_file (str): ƒê∆∞·ªùng d·∫´n ƒë·∫øn file CSV ƒë·∫ßu ra
        batch_count (int): S·ªë batch ƒë·ªÉ hi·ªÉn th·ªã trong th√¥ng b√°o
    """
    # S·ª≠ d·ª•ng lock ƒë·ªÉ tr√°nh ƒë·ª•ng ƒë·ªô khi nhi·ªÅu lu·ªìng c√πng ghi file
    with file_lock:
        # Ki·ªÉm tra xem file ƒë√£ t·ªìn t·∫°i ch∆∞a
        file_exists = os.path.exists(output_file)
        
        if file_exists:
            try:
                # ƒê·ªçc header c·ªßa file hi·ªán c√≥ ƒë·ªÉ ki·ªÉm tra c·∫•u tr√∫c
                existing_df = pd.read_csv(output_file)
                
                # So s√°nh c·∫•u tr√∫c c·ªôt
                existing_cols = set(existing_df.columns)
                new_cols = set(df.columns)
                
                if existing_cols != new_cols:
                    tqdm.write(f"Ph√°t hi·ªán c·∫•u tr√∫c c·ªôt kh√°c nhau. ƒêang k·∫øt h·ª£p d·ªØ li·ªáu...")
                    # K·∫øt h·ª£p DataFrame m·ªõi v·ªõi d·ªØ li·ªáu hi·ªán c√≥
                    combined_df = pd.concat([existing_df, df], ignore_index=True)
                    
                    # Ghi l·∫°i to√†n b·ªô file v·ªõi thanh ti·∫øn tr√¨nh
                    tqdm.write(f"ƒêang ghi {len(combined_df)} b·∫£n ghi v√†o file...")
                    combined_df.to_csv(output_file, index=False, encoding='utf-8-sig')
                    tqdm.write(f"‚úÖ ƒê√£ l∆∞u l√¥ th·ª© {batch_count}: {len(df)} b·∫£n ghi m·ªõi, t·ªïng {len(combined_df)} b·∫£n ghi")
                    return
                
                # N·∫øu c·∫•u tr√∫c c·ªôt gi·ªëng nhau, ch·ªâ c·∫ßn append
                df.to_csv(output_file, mode='a', header=False, index=False, encoding='utf-8-sig')
                tqdm.write(f"‚úÖ ƒê√£ l∆∞u l√¥ th·ª© {batch_count}: {len(df)} b·∫£n ghi")
                
            except Exception as e:
                tqdm.write(f"‚ùå L·ªói khi x·ª≠ l√Ω file: {e}")
                # N·∫øu c√≥ l·ªói, ghi ƒë√® file
                df.to_csv(output_file, index=False, encoding='utf-8-sig')
                tqdm.write(f"ƒê√£ ghi ƒë√® file v·ªõi l√¥ d·ªØ li·ªáu th·ª© {batch_count}")
        else:
            # N·∫øu file ch∆∞a t·ªìn t·∫°i, t·∫°o m·ªõi
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            tqdm.write(f"‚úÖ ƒê√£ t·∫°o file m·ªõi v√† l∆∞u {len(df)} b·∫£n ghi v√†o {output_file}")

# H√†m m·ªõi ƒë·ªÉ x·ª≠ l√Ω m·ªôt l√¥ URLs
def process_batch(urls_batch, output_file, batch_idx, total_batches):
    results = []
    batch_size = len(urls_batch)
    
    # C√†o d·ªØ li·ªáu cho t·∫•t c·∫£ URL trong l√¥
    for idx, url in enumerate(urls_batch):
        df_result, error = crawl_property_info(url, idx+1, batch_size)
        if not df_result.empty:
            results.append(df_result)
    
    # N·∫øu c√≥ k·∫øt qu·∫£, k·∫øt h·ª£p v√† l∆∞u
    if results:
        combined_df = pd.concat(results, ignore_index=True)
        append_to_csv(combined_df, output_file, batch_idx)

# H√†m m·ªõi ƒë·ªÉ x·ª≠ l√Ω ƒëa lu·ªìng
def process_with_multithreading(urls, num_threads=4, batch_size=10, output_file='property_data.csv'):
    """X·ª≠ l√Ω danh s√°ch URL v·ªõi ƒëa lu·ªìng"""
    total_urls = len(urls)
    
    # Chia danh s√°ch URL th√†nh c√°c l√¥
    batches = [urls[i:i + batch_size] for i in range(0, total_urls, batch_size)]
    num_batches = len(batches)
    
    print(f"\nüßµ S·ª≠ d·ª•ng {num_threads} lu·ªìng ƒë·ªÉ x·ª≠ l√Ω {total_urls} URL trong {num_batches} l√¥")
    
    # S·ª≠ d·ª•ng ThreadPoolExecutor ƒë·ªÉ ch·∫°y ƒëa lu·ªìng
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        # Submit c√°c t√°c v·ª• v√† l·∫•y c√°c Future objects
        futures = []
        for i, batch in enumerate(batches):
            future = executor.submit(process_batch, batch, output_file, i+1, num_batches)
            futures.append(future)
        
        # Theo d√µi ti·∫øn tr√¨nh v·ªõi tqdm
        with tqdm(total=num_batches, desc="Ti·∫øn tr√¨nh x·ª≠ l√Ω c√°c l√¥") as pbar:
            # ƒê·∫øm s·ªë l√¥ ƒë√£ ho√†n th√†nh
            completed = 0
            # Ki·ªÉm tra ti·∫øn tr√¨nh c·ªßa c√°c future
            while completed < num_batches:
                new_completed = sum(1 for f in futures if f.done())
                if new_completed > completed:
                    pbar.update(new_completed - completed)
                    completed = new_completed
                time.sleep(0.1)

if __name__ == "__main__":
    # File ƒë·∫ßu v√†o v√† ƒë·∫ßu ra
    input_csv = 'linkProduct.csv'
    output_csv = 'property_data.csv'
    batch_size = 10
    num_threads = 4  # S·ªë lu·ªìng x·ª≠ l√Ω ƒë·ªìng th·ªùi
    
    # Hi·ªÉn th·ªã ti√™u ƒë·ªÅ
    print("\n" + "="*70)
    print("üè† B·∫ÆT ƒê·∫¶U C√ÄO D·ªÆ LI·ªÜU B·∫§T ƒê·ªòNG S·∫¢N ƒêA LU·ªíNG")
    print("="*70 + "\n")
    
    # ƒê·ªçc c√°c URL t·ª´ file CSV
    print("üìÇ ƒêang ƒë·ªçc danh s√°ch URL...")
    urls = read_urls_from_csv(input_csv)
    
    if urls:
        print(f"üîç ƒê√£ t√¨m th·∫•y {len(urls)} URL ƒë·ªÉ x·ª≠ l√Ω, batch size: {batch_size}, threads: {num_threads}\n")
        
        # Hi·ªÉn th·ªã th·ªùi gian b·∫Øt ƒë·∫ßu
        start_time = time.time()
        
        # X·ª≠ l√Ω URL v·ªõi ƒëa lu·ªìng
        process_with_multithreading(urls, num_threads, batch_size, output_csv)
        
        # T√≠nh th·ªùi gian th·ª±c hi·ªán
        elapsed_time = time.time() - start_time
        hours, remainder = divmod(elapsed_time, 3600)
        minutes, seconds = divmod(remainder, 60)
        
        # Hi·ªÉn th·ªã th√¥ng b√°o ho√†n th√†nh
        print("\n" + "="*70)
        print(f"‚úÖ HO√ÄN TH√ÄNH! Th·ªùi gian: {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}")
        print(f"üìä D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o: {output_csv}")
        print("="*70)
    else:
        print("‚ùå Kh√¥ng c√≥ URL ƒë·ªÉ x·ª≠ l√Ω.")